#+TITLE: parser english transforms

* Overview
  Remember the basic ai that has 5 states and 5 possible actions. Now take
  what we learned from that and compare to an ai attempt with N states and
  N possible actions.

  The way our current parse tree works is we read in input such as
  : Hi how are you?
  and this is transformed to
  : (("Hi" :INTERJECTION :ADJECTIVE)
  :  ("how" :ADVERB :ADJECTIVE :CONJUNCTION :CONJUGATE :NOUN)
  :  ("are" :VERB :NOUN)
  :  ("you" :PRONOUN :DETERMINER)
  :  ("?"  :SYMBOL :INFLICTION))
  Stored in a database we will have to store it as "Hi how are you?", eg
  normal text. However for our lookup we need to reduce similar questions
  to the same representation. In lisp we call this normalization.

  The idea is to reduce many possible input phrases to a single phrase
  that means the same thing in the same context.

*** Data flow
    1) Input vie some method.
    2) tokenization of input.
       1) We parse for word/sentence structure. At some point we need to
          be able to have concrete sentences to work with.
       2) Assuming we have discrete sentences by this point we move on to
          the next step.
    3) Look up parts of speech for all elements.
    4) Transform and reduce the POS set for each sentence paying attention
       to context. This is the last N sentences spoken/parsed before it
       that are related. This step here is what this whole transform
       process is about.


 : he is very nice
 : A very nice person is he

 reverse is and he
 : A very nice person he is
 replace A with he is
 : He is very nice.


* classifying
  We need to realize right off that NLP is considerd an ai complete
  problem. It is very likely we are not going to be able to solve this to
  the ability of a human being. With that aside, some notes (by nixeagle)
  follow.



*** Nueral network notes
***** Perceptron
      Binary classifier that maps input x to output f(x) over matrix:
      f(x) = $\begin{cases}1  \text{if}w & \cdot x + b> 0
              \\0 & \text{else}\end{cases}$


      This provides for an up or down vote. b here is the bias, which is a
      constant term and $w \cdot x$ is our actual function mapping
      possible inputs to weights.
